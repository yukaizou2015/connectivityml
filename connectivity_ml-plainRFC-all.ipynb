{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"connectivity_ml-plainRFC-all.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"tags":[],"id":"8d0bb83e-ce84-4d64-8eca-9c51a25ffec0"},"source":["# Using functional connectivity from the HCP to predict individual characteristics\n","\n","Connectivity ML Group @ [Neurohackademy 2021](https://neurohackademy.org/)\n","\n","**Table of Contents**\n"," - [Import](#import)\n"," - [Data Analysis](#analysis)\n","   - [Analysis 1 (Tal)](#analysis-tpa)\n","   - [Analysis 2 (Debbie)](#analysis-db)\n"," - [Data Visualization](#vis)\n","   - [Visualization 1 (Kai)](#vis-yz)\n","   - [Visualization 2 (Priya)](#vis-pk)\n"," - [Optimization (Pradyumna)](#optim)"],"id":"8d0bb83e-ce84-4d64-8eca-9c51a25ffec0"},{"cell_type":"markdown","metadata":{"id":"ulV34LHNatFU"},"source":["<a name=\"import\"></a>\n","# But first, import!"],"id":"ulV34LHNatFU"},{"cell_type":"code","metadata":{"id":"3b4fc317-59af-420f-95e3-d3b7aa4e48ec"},"source":["# our core libraries\n","import os # (from PL)\n","import math\n","import numpy as np\n","import pandas as pd\n","import neuropythy as ny\n","import nibabel as nib\n","import ipyvolume as ipv\n","import random\n","import matplotlib.pyplot as plt\n","# import matplotlib as plt # (from PK)\n","import seaborn as sns\n","\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix # (from TPA and PL)\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import LabelEncoder # (from PL)\n","from sklearn import svm # (from TPA and PL)\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.linear_model import SGDRegressor, LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.utils import shuffle\n","#from sklearn.metrics import r2_score # (from DB)\n","#from sklearn.model_selection import train_test_split # (from DB)\n","#from sklearn import preprocessing # (from DB)\n","#from sklearn.metrics import confusion_matrix # (from DB)\n","\n","import shap  # package used to calculate Shap values # (from TPA and DB)\n","import itertools # (from PL)\n","import networkx as nx # (from YZ)"],"id":"3b4fc317-59af-420f-95e3-d3b7aa4e48ec","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"faf9b768-f82b-4531-a679-5e0b7e04e0e2"},"source":["# You need to configure neuropythy so that it knows what your\n","# HCP AWS S3 access key and secret are:\n","# key = 'YOURKEY'\n","# secret = 'YOURSECRET'\n","\n","ny.config['hcp_credentials'] = (key, secret)\n","\n","ny.config['hcp_auto_download'] = True\n","ny.config['hcp_auto_path'] = '~/hcp_data'"],"id":"faf9b768-f82b-4531-a679-5e0b7e04e0e2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"326428aa-16ff-4b62-a9f2-98f3ef6ea5a1"},"source":["<a name=\"analysis\"></a>\n","# Next, load the data.."],"id":"326428aa-16ff-4b62-a9f2-98f3ef6ea5a1"},{"cell_type":"markdown","metadata":{"id":"VUzwLBCFZ_dJ"},"source":["<a name=\"analysis-tpa\"></a>\n","## Analysis 1 (Tal)\n","\n","To `netmaps_df` we load \"netmaps\" which are subject-specific “parcellated connectomes” – for each subject, a nodes x nodes network matrix. See more [here](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP1200-DenseConnectome+PTN+Appendix-July2017.pdf).\n","\n","To `behavioral_df` we load the data keys. See more [here](https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release). "],"id":"VUzwLBCFZ_dJ"},{"cell_type":"code","metadata":{"id":"26bc00c4-a07a-4789-9c7f-b83c16b2ab1a"},"source":["N = 15 # number of ICAs - 15, 25, 50 ,100 ,200 , 300"],"id":"26bc00c4-a07a-4789-9c7f-b83c16b2ab1a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"074430fb-1f4e-4f53-a4c3-d6aedbab83a7"},"source":["netmaps_df = pd.read_csv('data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d'+str(N)+'_ts2/netmats2.txt', delim_whitespace=True,header=None)\n","print(\"Network-matrices data shape:\", netmaps_df.shape)\n","netmaps_df.head()"],"id":"074430fb-1f4e-4f53-a4c3-d6aedbab83a7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fc5d3c6-7ae8-436e-bdbc-751ce857565e"},"source":["behavioral_df = pd.read_csv('data/connectivityml/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n","print(\"Behaviora data shape:\", behavioral_df.shape)\n","behavioral_df.head()"],"id":"8fc5d3c6-7ae8-436e-bdbc-751ce857565e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75939302-5820-423e-9861-1764c005a27c"},"source":["We have netmaps for 1003 subjects so we will need to filter `behavioral_df` a little.\n","\n","To `subjectsID_df` we load the ordered list of all subjects with complete rfMRI data (recon 1 + recon2) included in this PTN release"],"id":"75939302-5820-423e-9861-1764c005a27c"},{"cell_type":"code","metadata":{"id":"af7312c7-68a1-4307-a857-cc011cd8b962"},"source":["subjectsID_df = pd.read_csv('data/connectivityml/HCP_PTN1200/subjectIDs.txt',header=None,names=[\"Subject\"])\n","print(\"Subjects ID data shape:\", subjectsID_df.shape)\n","subjectsID_df.head()"],"id":"af7312c7-68a1-4307-a857-cc011cd8b962","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5b5c6a4-043c-4a1b-bcf1-b69317e468bc"},"source":["We can see that this corresponds to the number of netmaps we have."],"id":"b5b5c6a4-043c-4a1b-bcf1-b69317e468bc"},{"cell_type":"code","metadata":{"id":"80235878-c331-4f35-8d81-c1f3460abb9a"},"source":["filter_behavioral_df = subjectsID_df.merge(behavioral_df, on='Subject', how='inner')\n","\n","print(\"Filtered behaviora data shape:\", filter_behavioral_df.shape)\n","filter_behavioral_df.head()"],"id":"80235878-c331-4f35-8d81-c1f3460abb9a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"303e2bef-67b1-4ac2-b367-a1885c39165a"},"source":["### Pre-process features matrix"],"id":"303e2bef-67b1-4ac2-b367-a1885c39165a"},{"cell_type":"code","metadata":{"id":"339377cf-c709-465c-a10c-2c4c6e1ec327"},"source":["netmapsX_df = pd.DataFrame(data = netmaps_df, columns = range(N*N))\n","netmapsX_df = netmapsX_df.T.drop_duplicates(keep='first').T\n","netmapsX_df = netmapsX_df.T.drop_duplicates(keep='last').T\n","X = netmapsX_df\n","print(\"Features matrix shape:\", X.shape)\n","X.head()"],"id":"339377cf-c709-465c-a10c-2c4c6e1ec327","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0b9fc56c-a343-4b1f-a5c4-2de5997907c7"},"source":["### Pre-process predicted values\n","\n","Here we are going to foucs on the subject gender."],"id":"0b9fc56c-a343-4b1f-a5c4-2de5997907c7"},{"cell_type":"code","metadata":{"id":"aebe74da-cd7d-4bac-b929-8d546e82123f"},"source":["filter_behavioral_df['Gender_i']=np.zeros(shape=(subjectsID_df.shape))\n","filter_behavioral_df.Gender_i = pd.factorize(filter_behavioral_df.Gender)[0] # Encode the object as an enumerated type or categorical variable.\n","y_gender = filter_behavioral_df.Gender_i # Gender of Subject\n","print(\"y_gender shape:\", y_gender.shape)\n","\n","filter_behavioral_df['Gender_i'].groupby(filter_behavioral_df['Gender']).unique().apply(pd.Series).rename(columns={0:'Labels'}).sort_values(by='Labels')"],"id":"aebe74da-cd7d-4bac-b929-8d546e82123f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fc95d1f-d3cc-48ed-8f76-868b08ce27a7"},"source":["fig, ax = plt.subplots()\n","sns.histplot(y_gender, ax=ax)\n","ax.set_title(\"Gender of Subject\")\n","fig.tight_layout()"],"id":"6fc95d1f-d3cc-48ed-8f76-868b08ce27a7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"90a4cdb9-4090-40d3-af5d-28f3baf14b44"},"source":["### Time for Random forests!\n","\n","Data and estimators exploration was done in a separate notebook..\n","\n","#### [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n","\n","A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n","\n","Here we will prdict the gender of subjects in our dataset from our network-matrices data has gave us the best results..."],"id":"90a4cdb9-4090-40d3-af5d-28f3baf14b44"},{"cell_type":"code","metadata":{"id":"e1bd76e4-94e6-4d87-9508-499c834ebda4"},"source":["xtrain, xtest, ytrain, ytest = train_test_split(X, y_gender, test_size=0.2,stratify=y_gender,random_state=1)"],"id":"e1bd76e4-94e6-4d87-9508-499c834ebda4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fa8858b-3164-4b22-bc03-568cdf1f5e6e"},"source":["rf = RandomForestClassifier(random_state=1)\n","\n","rf.fit(xtrain, ytrain) # Build a forest of trees from the training set (X, y).\n","\n","score = rf.score(xtest, ytest) # Return the mean accuracy on the given test data and labels.\n","print(\"Test set score: \", score) \n","\n","ypred = rf.predict(xtest) # Predict class for X."],"id":"4fa8858b-3164-4b22-bc03-568cdf1f5e6e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"523c7b72-0781-4ad5-aac4-553d70ccad58"},"source":["# View confusion matrix for test data and predictions\n","plot_confusion_matrix(rf, xtest, ytest) # Plot Confusion Matrix.\n","plt.savefig('output/confusion_matrix_'+str(N)+'.png')"],"id":"523c7b72-0781-4ad5-aac4-553d70ccad58","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3b139368-2e37-49f1-9b1d-dac6a36e2e42"},"source":["cr = classification_report(ytest, ypred,output_dict=True) # Build a text report showing the main classification metrics.\n","sns_plot = sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True) # plot scikit-learn classification report\n","sns_plot.figure.savefig(\"output/classification_report_\"+str(N)+\".png\")"],"id":"3b139368-2e37-49f1-9b1d-dac6a36e2e42","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ee545cbc-2457-49cd-96a4-8cfad5d22e37"},"source":["#### Visualizing trees\n","We can plot individual decision trees - "],"id":"ee545cbc-2457-49cd-96a4-8cfad5d22e37"},{"cell_type":"code","metadata":{"id":"a51dd795-c97d-4019-8c65-4392fb9ef964"},"source":["from sklearn.tree import export_graphviz\n","from subprocess import call\n","from IPython.display import Image, display\n","\n","def plot_graphviz_tree(tree):\n","    \"\"\"\n","    Helper function that takes a tree as input, calls sklearn's export_graphviz\n","    function to generate an image of the tree using graphviz, and then\n","    plots the result in-line.\n","    \"\"\"\n","    export_graphviz(tree, out_file='tree.dot', max_depth=3, filled=True,\n","                    feature_names=X.columns, impurity=False, rounded=True,\n","                    proportion=False, precision=2);\n","\n","    call(['dot', '-Tpng', 'tree.dot', '-o', 'output/tree_'+str(N)+'.png', '-Gdpi=600'])\n","    display(Image(filename = 'output/tree_'+str(N)+'.png'));"],"id":"a51dd795-c97d-4019-8c65-4392fb9ef964","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a063b39-ba0f-48be-b56d-a3af22f67501"},"source":["# First tree in the forest\n","plot_graphviz_tree(rf.estimators_[0]);"],"id":"8a063b39-ba0f-48be-b56d-a3af22f67501","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e93f20a0-5368-4bbb-89bb-0fdd857f62b4"},"source":["#### Interpreting random forests\n","\n","##### Feature importances\n","\n","Unlike regression-based methods, random forests don't have linear coefficientsso we look at the feature importances, to tell us how each feature contributes to the overall prediction."],"id":"e93f20a0-5368-4bbb-89bb-0fdd857f62b4"},{"cell_type":"code","metadata":{"id":"a9490afb-09cc-4bad-991b-fab07af60cb7"},"source":["f = open('output/feature_importances_'+'_'+str(N)+'.txt', \"w\")\n","f.write(str(pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)))\n","f.close()\n","\n","# plot feature importances\n","(pd.Series(rf.feature_importances_, index=X.columns)\n","   .nlargest(10)\n","   .plot(kind='bar',title=\"Feature importances for \"+str(N)+ \" ICA's\"))\n","plt.savefig('output/feature_importances_'+str(N)+'.png')"],"id":"a9490afb-09cc-4bad-991b-fab07af60cb7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3b931bfb-dcd4-40cf-b9e7-ba6567805726"},"source":["##### SHAP values\n","To interpret our results we will look at the SHAP values of our model.\n","See more [here](https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a).\n","\n","SHAP feature importance is an alternative to standard feature importance based on magnitude of feature attributions. The feature importance is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot.\n","\n","The SHAP summary plot is made of all the dots in the test data. Showing:\n","- **Feature importance:** Variables are ranked in descending order.\n","- **Impact:** The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n","- **Original value:** Color shows whether that variable is high (in red) or low (in blue) for that observation.\n"],"id":"3b931bfb-dcd4-40cf-b9e7-ba6567805726"},{"cell_type":"code","metadata":{"id":"ffc29fdb-57d3-4333-ad92-2e741f936898"},"source":["# Create object that can calculate shap values\n","explainer = shap.TreeExplainer(rf)\n","\n","# calculate shap values. This is what we will plot.\n","# Calculate shap_values for all of xtest rather than a single row, to have more data for plot.\n","shap_values = explainer.shap_values(xtest)\n","\n","# Make plot\n","fig = shap.summary_plot(shap_values[1], xtest, show=False)\n","plt.savefig('output/summary_plot_xtest_'+str(N)+'.png')"],"id":"ffc29fdb-57d3-4333-ad92-2e741f936898","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7ce6a5f-5963-4b41-9024-af6c7e832a8d"},"source":["fig = shap.summary_plot(shap_values, xtest, show=False,plot_type='bar')\n","plt.savefig('output/SHAP_feature_importances_xtest_'+str(N)+'.png')"],"id":"f7ce6a5f-5963-4b41-9024-af6c7e832a8d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c67658a9-a5df-4707-a2aa-c40db4ca6214"},"source":["#### Single subject interpretation\n","We will look at SHAP values for a random row of the dataset.\n","\n","##### SHAP values\n","For context, we'll look at the raw predictions before looking at the SHAP values."],"id":"c67658a9-a5df-4707-a2aa-c40db4ca6214"},{"cell_type":"code","metadata":{"id":"28411b66-fb7a-4200-8783-4abc9c5648e3"},"source":["row_to_show = 5\n","data_for_prediction = xtest.loc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n","data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n","\n","rf.predict_proba(data_for_prediction_array)"],"id":"28411b66-fb7a-4200-8783-4abc9c5648e3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dffefd2-c8dc-455a-a303-eacad55d9d79"},"source":["print(\"Subject evaluated:\",subjectsID_df.Subject[row_to_show])"],"id":"9dffefd2-c8dc-455a-a303-eacad55d9d79","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6447328a-4c58-4154-9aeb-27f5f5c31a9d"},"source":["# Create object that can calculate shap values\n","explainer = shap.TreeExplainer(rf)\n","\n","# Calculate Shap values\n","shap_values = explainer.shap_values(data_for_prediction)\n","shap.initjs()\n","\n","shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=False)\n","# plt.savefig('force_plot_hcp_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.png')"],"id":"6447328a-4c58-4154-9aeb-27f5f5c31a9d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"11c07ff3-e721-43b9-b6f9-e1419f780a47"},"source":["The SHAP values of all features sum up to explain why the prediction was different from the baseline. This allows us to decompose a prediction in a graph like this where:\n","\n","- The **output value** is the prediction for that observation (the prediction of the subject evluated).\n"," - The **base value** is the value that would be predicted if we did not know any features for the current output (the mean prediction, or mean(yhat)).\n"," - **Red/blue:** Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.\n"," \n","##### TOP 5 Most important ICA's"],"id":"11c07ff3-e721-43b9-b6f9-e1419f780a47"},{"cell_type":"code","metadata":{"id":"1d1588e0-56c4-4129-9daf-590afb7ea7ba"},"source":["vals= np.abs(shap_values).mean(0)\n","feature_importance = pd.DataFrame(list(zip(xtest.columns,vals)),columns=['netmat_col','feature_importance_vals'])\n","feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n","feature_importance.head()"],"id":"1d1588e0-56c4-4129-9daf-590afb7ea7ba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cdd31dd-50e0-48d2-8543-6fca0eaa24b4"},"source":["edge_df = pd.DataFrame(columns = ['netmat_col','Node1', 'Node2', 'Weight'])\n","countlist = list() # if node1,node2 weight is saved no need to save node2,node1\n","\n","for index, row in tqdm(netmaps_df.iterrows(), total=netmaps_df.shape[0]):\n","    netmat_col = 0\n","    if index == row_to_show:\n","        row2mat = row.values.reshape(N,N)\n","        for node1 in range(N):\n","            for node2 in range(N):\n","                if node1!=node2:\n","                    if (node2, node1) not in countlist:\n","                        countlist.append((node1, node2))\n","                        curr_edge = {'netmat_col': netmat_col, 'Node1': node1, 'Node2': node2, 'Weight':row2mat[node1][node2]}\n","                        edge_df = edge_df.append(curr_edge, ignore_index = True)\n","                netmat_col = netmat_col + 1"],"id":"2cdd31dd-50e0-48d2-8543-6fca0eaa24b4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2571ad6-12ed-4e5b-a6d0-02e721e13122"},"source":["f = open('output/top5_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.txt', \"w\")\n","\n","for i in range(5):\n","    feature = feature_importance.netmat_col.values[i]\n","    print(\"Important feature #: \",feature)\n","    f.write(\"Important feature #: \" + str(feature)+\"\\n\")\n","    ICA_Node1 = edge_df.loc[edge_df['netmat_col'] == feature, 'Node1']\n","    ICA_Node2 = edge_df.loc[edge_df['netmat_col'] == feature, 'Node2']\n","\n","    print(\"Node 1:\",ICA_Node1.iloc[0])\n","    f.write(\"Node 1: \" + str(ICA_Node1.iloc[0])+\"\\n\")\n","    print(\"Node 2:\",ICA_Node2.iloc[0])\n","    f.write(\"Node 2: \" + str(ICA_Node2.iloc[0])+\"\\n\")\n","f.close()"],"id":"a2571ad6-12ed-4e5b-a6d0-02e721e13122","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9990cff1-c03e-417a-99c8-ba6db5dfe9c1"},"source":["Knowing the ICA's that are part of the most important feature we can plot them on our single subject brain"],"id":"9990cff1-c03e-417a-99c8-ba6db5dfe9c1"},{"cell_type":"code","metadata":{"id":"aad6774d-f6d6-40e4-891f-cee62e01c4c1"},"source":["# Get a sample HCP subject:\n","sub = ny.hcp_subject(subjectsID_df.Subject[row_to_show])\n","sub"],"id":"aad6774d-f6d6-40e4-891f-cee62e01c4c1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1585ddd9-9f6b-42e4-9f1c-291798da2673"},"source":["# Load the CIFTI file:\n","cii_filename = '~/data/connectivityml/HCP_PTN1200/groupICA/groupICA_3T_HCP1200_MSMAll_d'+str(N)+'.ica/melodic_IC.dscalar.nii'\n","cii_obj = ny.load(cii_filename)\n","\n","# Split the CIFTI object into hemisphere/subvoxel data:\n","(lh_data, rh_data, subvox_data) = ny.hcp.cifti_split(cii_obj)\n","\n","# These data should be (N(data-points) x vertices)\n","lh_data.shape"],"id":"1585ddd9-9f6b-42e4-9f1c-291798da2673","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6570e87-ee28-4383-93de-b3eab9caf9c7"},"source":["# sub.lh and sub.rh are the \"native\" (FreeSurfer) hemispheres;\n","# sub.hemis['lh_LR32k'] and sub.hemis['rh_LR32k'] are the\n","# HCP subject-aligned fs_LR hemispheres (with 32k resolution).\n","lh_hemi_native = sub.lh\n","rh_hemi_native = sub.rh\n","\n","# The 32492 size indicates this is a 32k LR hemisphere:\n","lh_hemi = sub.hemis['lh_LR32k']\n","rh_hemi = sub.hemis['rh_LR32k']"],"id":"a6570e87-ee28-4383-93de-b3eab9caf9c7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"ea1aa2b6-bf67-4851-8dfb-ad78c567717b"},"source":["**ICA 1**"],"id":"ea1aa2b6-bf67-4851-8dfb-ad78c567717b"},{"cell_type":"code","metadata":{"id":"358687d7-4423-47d0-9782-a6d9a5af23ee"},"source":["# We can make an ipyvolume figure to plot both hemispheres on:\n","fig = ipv.figure()\n","# Then plot each hemisphere using whichever ICA component we\n","# want to visualize:\n","\n","ICA_Node1 = int(edge_df.loc[edge_df['netmat_col'] == feature_importance.netmat_col.values[0], 'Node1'])\n","ny.cortex_plot(lh_hemi, surface='inflated', color=lh_data[ICA_Node1], cmap='hot', figure=fig)\n","ny.cortex_plot(rh_hemi, surface='inflated', color=rh_data[ICA_Node1], cmap='hot', figure=fig)"],"id":"358687d7-4423-47d0-9782-a6d9a5af23ee","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7c08b73-3852-456d-bdbe-269ce071c14e"},"source":["ipv.pylab.save('output/most_important_ICA1_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.html',title='Most important feature ICA 1')"],"id":"f7c08b73-3852-456d-bdbe-269ce071c14e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"bef74681-caca-4c3b-9bba-48225091d407"},"source":["**ICA 2**"],"id":"bef74681-caca-4c3b-9bba-48225091d407"},{"cell_type":"code","metadata":{"id":"23b91b7a-95d0-4437-87e2-25141ee7b8ec"},"source":["# We can make an ipyvolume figure to plot both hemispheres on:\n","fig = ipv.figure()\n","# Then plot each hemisphere using whichever ICA component we\n","# want to visualize:\n","\n","ICA_Node2 = int(edge_df.loc[edge_df['netmat_col'] == feature_importance.netmat_col.values[0], 'Node2'])\n","ny.cortex_plot(lh_hemi, surface='inflated', color=lh_data[ICA_Node2], cmap='hot', figure=fig)\n","ny.cortex_plot(rh_hemi, surface='inflated', color=rh_data[ICA_Node2], cmap='hot', figure=fig)"],"id":"23b91b7a-95d0-4437-87e2-25141ee7b8ec","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adf994a1-a6d5-4161-8d08-7d20fcd92951"},"source":["ipv.pylab.save('output/most_important_ICA2_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.html',title='Most important feature ICA 2')"],"id":"adf994a1-a6d5-4161-8d08-7d20fcd92951","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbr3B-Q0aJqV"},"source":["<a name=\"analysis-db\"></a>\n","## Analysis 2 (Debbie)"],"id":"gbr3B-Q0aJqV"},{"cell_type":"markdown","metadata":{"id":"7PdGP0nQaVvM"},"source":["### Read in demographics file"],"id":"7PdGP0nQaVvM"},{"cell_type":"code","metadata":{"id":"FwEu9BFuaeke"},"source":["df_dem = pd.read_csv(\"~/data/connectivityml/unrestricted_pkalra_7_26_2021_17_39_25.csv\") "],"id":"FwEu9BFuaeke","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJmtZQpZafTM"},"source":["df_dem.head()"],"id":"JJmtZQpZafTM","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaTfYxOjaiJO"},"source":["print(df_dem.shape)\n","print(df_dem.columns.shape)"],"id":"yaTfYxOjaiJO","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTC-_KIpangW"},"source":["In the demographics file we found:\n","\n","- 1206 subjects\n","- 582 demographics values"],"id":"PTC-_KIpangW"},{"cell_type":"markdown","metadata":{"id":"g7aLqNbnaxUv"},"source":["### Read in partial correlations connectivity file:\n","\n"," path to netmats2 for d100: `data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d100_ts2/netmats2.txt`"],"id":"g7aLqNbnaxUv"},{"cell_type":"code","metadata":{"id":"mX_PQQ9bakGP"},"source":["df_conn_symm_d300 = pd.read_csv(\"~/data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d300_ts2/netmats2.txt\", sep=\" \", header=None) \n","df_conn_symm_d100 = pd.read_csv(\"~/data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d100_ts2/netmats2.txt\", sep=\" \", header=None) \n","df_conn_symm_d15 = pd.read_csv(\"~/data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d15_ts2/netmats2.txt\", sep=\" \", header=None) "],"id":"mX_PQQ9bakGP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1buqd876a_Dx"},"source":["print(df_conn_symm_d300.shape)\n","print(df_conn_symm_d100.shape)\n","print(df_conn_symm_d15.shape)"],"id":"1buqd876a_Dx","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABuR3U6GbCL7"},"source":["In the connectivity files we found:\n","\n","- 1003 subjects\n","- ICA 300: 300x300 = 90000 connectivity partial correlations\n","- ICA 100: 100x100 = 10000 connectivity partial correlations\n","- ICA 15: 15x15 = 225 connectivity partial correlations"],"id":"ABuR3U6GbCL7"},{"cell_type":"code","metadata":{"id":"iUOf6ce-bJ77"},"source":["#Code from Pradyumna to delete lower triangle of connectivity matrix\n","\n","def pick_upper_triangle(df_conn_symm):\n","    p = df_conn_symm.shape[0]\n","    N = np.sqrt(df_conn_symm.shape[1]).astype(int)\n","\n","    mat = np.transpose(df_conn_symm.values.reshape(p,N,N),[1,2,0])\n","    indx = np.triu_indices(N,1)\n","    mat2 = np.transpose(mat[indx],[1,0])\n","    print(mat2.shape)\n","    df_conn = pd.DataFrame(data=mat2)\n","    return df_conn, indx"],"id":"iUOf6ce-bJ77","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmajzTYPbLBB"},"source":["df_conn_d300, indx_d300 = pick_upper_triangle(df_conn_symm_d300)\n","df_conn_d100, indx_d100 = pick_upper_triangle(df_conn_symm_d100)\n","df_conn_d15, indx_d15 = pick_upper_triangle(df_conn_symm_d15)"],"id":"tmajzTYPbLBB","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHjEXKR_bOX5"},"source":["### Read in subject ID file:\n","##### --> want to make sure we filter the demographics and netmats file so that we only include shared participants\n","##### --> doc says that the netmats rows correspond to the subjectids in subjectIDs.txt (from Priya)"],"id":"yHjEXKR_bOX5"},{"cell_type":"code","metadata":{"id":"Vfaqk6dXbRg4"},"source":["df_id = pd.read_csv(\"~/data/connectivityml/HCP_PTN1200/subjectIDs.txt\", sep=\" \", header=None) \n","df_id.rename(columns={ df_id.columns[0]: \"Subject\" },inplace = True)"],"id":"Vfaqk6dXbRg4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9rZJnbwAbTs4"},"source":["print(df_id.shape)\n","df_id.head()"],"id":"9rZJnbwAbTs4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HDmq6UaKbYuX"},"source":["### Create a new connectivity \"triangle\" files with subject IDs included:"],"id":"HDmq6UaKbYuX"},{"cell_type":"code","metadata":{"id":"JyZq4GrUbVPW"},"source":["df_conn_id_d300 = pd.concat([df_id, df_conn_d300], axis=1)\n","df_conn_id_d100 = pd.concat([df_id, df_conn_d100], axis=1)\n","df_conn_id_d15 = pd.concat([df_id, df_conn_d15], axis=1)\n","\n","print(df_conn_id_d15.shape)\n","df_conn_id_d15.head()"],"id":"JyZq4GrUbVPW","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONYH6AzebgIS"},"source":["### Create new dem file with only netmats subjects, ordered in the same way:"],"id":"ONYH6AzebgIS"},{"cell_type":"code","metadata":{"id":"0k_FeznJbcKO"},"source":["df_dem_id= pd.merge_ordered(df_id, df_dem, left_by='Subject')\n","print(df_dem_id.shape)\n","df_dem_id.head()"],"id":"0k_FeznJbcKO","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-liBvCCmblhq"},"source":["Now we have two types of files: df_conn_id_d* (netmats upper triangle info, 3x) and df_dem_id (demographics info, 1x)\n"," - both only containing 1003 subjects shared between dem and netmats files\n"," - both ordered the same way\n"," - both with \"Subject\" as the first column"],"id":"-liBvCCmblhq"},{"cell_type":"markdown","metadata":{"id":"j82YAiBHbtrD"},"source":["### We can start building ML models now! \n","##### --> keep in mind that not all demographics values exist for all participants (some are na)\n","##### --> start with creating a function that can produce a x,y for a desired outcome that has lost all na rows for said outcome"],"id":"j82YAiBHbtrD"},{"cell_type":"code","metadata":{"id":"7gZcaoHbbvDc"},"source":["#function to spit out x and y for ML models depending on what we want to predict\n","#filters out na values\n","def define_X_y(desired_y,conn_id,dem_id=df_dem_id):\n","    X = conn_id[dem_id[desired_y].notnull()].drop(['Subject'], axis=1)\n","    temp_y = dem_id[dem_id[desired_y].notnull()]\n","    y = temp_y[desired_y]\n","    print(X.shape)\n","    print(y.shape)\n","    return X,y"],"id":"7gZcaoHbbvDc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HcnvJnsbxd0"},"source":["X_gend_d300 ,y_gend_d300 = define_X_y(\"Gender\", df_conn_id_d300)\n","X_gend_d100 ,y_gend_d100 = define_X_y(\"Gender\", df_conn_id_d100)\n","X_gend_d15 ,y_gend_d15 = define_X_y(\"Gender\", df_conn_id_d15)"],"id":"2HcnvJnsbxd0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRDEL70fby_K"},"source":["rf_d300 = RandomForestClassifier()\n","rf_d100 = RandomForestClassifier()\n","rf_d15 = RandomForestClassifier()\n","\n","rf_d300.fit(X_gend_d300, y_gend_d300)\n","rf_d100.fit(X_gend_d100, y_gend_d100)\n","rf_d15.fit(X_gend_d15, y_gend_d15)"],"id":"YRDEL70fby_K","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VGnt7q4xb9BZ"},"source":["### First we get the accuracy of our default classifier for the 3 ICA partitions:\n","##### --> use 5-fold cross-validation"],"id":"VGnt7q4xb9BZ"},{"cell_type":"code","metadata":{"id":"tlioKVOQb01Q"},"source":["r2_cv_d300 = cross_val_score(rf_d300, X_gend_d300, y_gend_d300, cv=5)\n","r2_cv_d100 = cross_val_score(rf_d100, X_gend_d100, y_gend_d100, cv=5)\n","r2_cv_d15 = cross_val_score(rf_d15, X_gend_d15, y_gend_d15, cv=5)"],"id":"tlioKVOQb01Q","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPtmk1Krb_rF"},"source":["print(r2_cv_d300, \"Average accuracy:\", np.mean(r2_cv_d300))\n","print(r2_cv_d100, \"Average accuracy:\", np.mean(r2_cv_d100))\n","print(r2_cv_d15, \"Average accuracy:\", np.mean(r2_cv_d15))"],"id":"WPtmk1Krb_rF","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUTxitw7cBVv"},"source":["X_gend_d15.shape"],"id":"hUTxitw7cBVv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5eCsvOScDBL"},"source":["y_gend_d15.shape"],"id":"D5eCsvOScDBL","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gN0f5OsKcIRp"},"source":["### Compare SHAP analysis results for the 3 ICA partitions & see how much variation between folds there is:\n","##### Now we do 5-fold cross-validation by hand to look at how the most important features according to SHAP analysis change"],"id":"gN0f5OsKcIRp"},{"cell_type":"code","metadata":{"id":"IEbO61WicE22"},"source":["def cross_val_with_shap(rf, X_gend, y_gend,cv=5):\n","    #randomize indices of subjects\n","    #in loop, reorder subjects, do partition into test/train, do shap plot, spit out top shap results\n","    \n","    X,y = shuffle(X_gend, y_gend)\n","    \n","    #comment out for sanity check that shuffling X,y independently leads to prediction no better than chance\n","    #X = shuffle(X_gend, random_state=2)\n","    #y = shuffle(y_gend, random_state=1)\n","    \n","    fold_size = np.floor_divide(y_gend.shape[0],cv)\n","    \n","    split_indices = []\n","    for i in range(1,cv):\n","        temp=fold_size*i\n","        split_indices.append(temp.astype(int))\n","    \n","    X_split = np.vsplit(X,split_indices)\n","    y_split = np.hsplit(y,split_indices)\n","    \n","    shap_values_cv = []\n","\n","    for j in range(0,cv):\n","        train_indices = list(np.arange(5))\n","        train_indices.remove(j)\n","        \n","        X_test = X_split[j]\n","        X_train = [np.array(e) for i, e in enumerate(X_split) if i in train_indices]\n","        X_train = np.vstack(X_train)\n","        \n","        y_test = np.array(y_split[j])\n","        y_train = [np.array(e) for i, e in enumerate(y_split) if i in train_indices]\n","        y_train = np.hstack(y_train)\n","        \n","        rf = RandomForestClassifier()\n","        rf.fit(X_train,y_train)\n","        train_acc = rf.score(X_train, y_train)\n","        test_acc = rf.score(X_test, y_test)\n","        \n","        print(test_acc)\n","        \n","        shap_values = shap.TreeExplainer(rf).shap_values(X_train)\n","        shap.summary_plot(shap_values, X_train)\n","        shap_values_cv.append(shap_values)\n","        \n","    return shap_values_cv"],"id":"IEbO61WicE22","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__UayDK3cLqU"},"source":["shap_values_cv = cross_val_with_shap(rf_d15,X_gend_d15,y_gend_d15)"],"id":"__UayDK3cLqU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rxr_4rpZcOif"},"source":["shap_values_cv = cross_val_with_shap(rf_d300,X_gend_d300,y_gend_d300)"],"id":"Rxr_4rpZcOif","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAoKWFTGcRKZ"},"source":["X_gend_d300.shape"],"id":"VAoKWFTGcRKZ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X73LrZiDcUwp"},"source":["print(indx_d300[0][43928])\n","print(indx_d300[1][43928])"],"id":"X73LrZiDcUwp","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdvuTay9cWgg"},"source":["print(indx_d300[0][5350])\n","print(indx_d300[1][5350])"],"id":"KdvuTay9cWgg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bWS1EnWucYub"},"source":["print(indx_d300[0][9358])\n","print(indx_d300[1][9358])"],"id":"bWS1EnWucYub","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9dfz9INcifK"},"source":["print(indx_d300[0][9404])\n","print(indx_d300[1][9404])"],"id":"u9dfz9INcifK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL3QVuNiclPn"},"source":["print(indx_d300[0][32328])\n","print(indx_d300[1][32328])"],"id":"TL3QVuNiclPn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KK19J9KFcnXP"},"source":["print(indx_d15[0][84])\n","print(indx_d15[1][84])"],"id":"KK19J9KFcnXP","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttEkq-kJbfIk"},"source":["<a name=\"vis\"></a>\n","# Then, visualize!"],"id":"ttEkq-kJbfIk"},{"cell_type":"markdown","metadata":{"id":"NjDqYhELbjAH"},"source":["<a name=\"vis-yz\"></a>\n","## Visualization 1 (Kai)\n","\n","### Brain-free network model using Gephi\n","\n","See more about Gephi [here](https://gephi.org/). The following primarily follows [this instruction](https://towardsdatascience.com/from-csv-to-github-pages-in-5-steps-publishing-an-interactive-social-network-of-the-marvel-7b8374bf44fb). There is a tutorial about using the `GephiStreamer` python package [here](https://pypi.org/project/GephiStreamer/), but that has to be run with Graph streaming plugin enabled, although there is an [instruction](https://tbgraph.wordpress.com/2017/04/01/neo4j-to-gephi/) it's not straightforward to set this up in NeuroHackademy Jupyterhub."],"id":"NjDqYhELbjAH"},{"cell_type":"code","metadata":{"id":"3346821a-069b-4400-9d61-b4b05e8f538e"},"source":["edge_df['weight'] = 1\n","\n","for ind in (feature_importance.netmat_col[:5].index):\n","    print(ind)\n","    edge_df.loc[ind-1,'weight'] = 5\n","edge_df"],"id":"3346821a-069b-4400-9d61-b4b05e8f538e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0JZ9sk4rhrgm"},"source":["Make sure that the weights from the top features are changed correctly for visualization:"],"id":"0JZ9sk4rhrgm"},{"cell_type":"code","metadata":{"id":"mA1WEE1Nhk3-"},"source":["edge_df.loc[edge_df['netmat_col'].isin(feature_importance.netmat_col[:5])]"],"id":"mA1WEE1Nhk3-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Av_u6KBviD1A"},"source":["G=nx.from_pandas_edgelist(edge_df, 'Node1', 'Node2', edge_attr='weight')"],"id":"Av_u6KBviD1A","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4Z9rgnTiXUq"},"source":["# number of ICAs - 15, 25, 50 ,100 ,200 , 300\n","\n","nx.write_gexf(G, \"/home/jovyan/Hackathon/code/tpatpa/connectivityml/gephi-15-top5-weighted.gexf\")"],"id":"j4Z9rgnTiXUq","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mwPoY1ljLDZ"},"source":["![Top 5 features from 15 ICAs, labelled](visualization-gephi/gephi-15-top5-weighted-labelled.png)"],"id":"2mwPoY1ljLDZ"},{"cell_type":"markdown","metadata":{"id":"KHLFeMISbr8y"},"source":["<a name=\"vis-pk\"></a>\n","## Visualization 2 (Priya)\n","\n","### Brain Mapping using Neuropythy"],"id":"KHLFeMISbr8y"},{"cell_type":"markdown","metadata":{"id":"FAu2M8ogP_4t"},"source":["Set the number of ICAs you are using in the next cell (replace X with 15, 25, 50, 100 or 200)"],"id":"FAu2M8ogP_4t"},{"cell_type":"code","metadata":{"id":"cKxU0y6YP9GK"},"source":["icanum = 15"],"id":"cKxU0y6YP9GK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsAuhcShQGX7"},"source":["sub = ny.hcp_subject(100206)\n","lh_hemi_native = sub.lh\n","rh_hemi_native = sub.rh\n","lh_hemi_LR32k = sub.hemis['lh_LR32k']\n","rh_hemi_LR32k = sub.hemis['rh_LR32k']"],"id":"jsAuhcShQGX7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPHc5svJQJS2"},"source":["cii_filename = '~/data/connectivityml/HCP_PTN1200/groupICA/groupICA_3T_HCP1200_MSMAll_d'+str(icanum)+'.ica/melodic_IC.dscalar.nii'\n","cii_obj = ny.load(cii_filename)"],"id":"bPHc5svJQJS2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9YqfHHxQMcb"},"source":["(lh_data, rh_data, subvox_data) = ny.hcp.cifti_split(cii_obj)"],"id":"a9YqfHHxQMcb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuEv2Xb3QR_k"},"source":["Choose a percentile--how many vertices from the original ICA map do you want to keep? (90th, 95th or 99th work well)"],"id":"HuEv2Xb3QR_k"},{"cell_type":"code","metadata":{"id":"KvK9QzbwQPoj"},"source":["cpercentile = 95"],"id":"KvK9QzbwQPoj","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1qxxDRgQU_m"},"source":["lh_thresh_list = list()\n","for i in range(0,icanum):\n","    myrow = lh_data[i]\n","    lh_thresh_list.append(np.nanpercentile(myrow, cpercentile))"],"id":"o1qxxDRgQU_m","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HU7gR_JPQX6r"},"source":["rh_thresh_list = list()\n","for i in range(0,icanum):\n","    myrow = rh_data[i]\n","    #print(i, mymax, mymin, np.nanpercentile(myrow, 95))\n","    rh_thresh_list.append(np.nanpercentile(myrow, cpercentile))\n","    #print(np.nanpercentile(myrow, 90))\n","    #plt.hist(lh_data[i])"],"id":"HU7gR_JPQX6r","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8BvmDyl2QcUG"},"source":["lh_labels = np.zeros(32492, dtype='int')\n","for (ii,myrow) in enumerate(lh_data):\n","    mycutoff = lh_thresh_list[ii]\n","    mylabel = ii + 1 # we want labels to start at 1, not 0\n","    lh_labels[myrow > mycutoff] = mylabel"],"id":"8BvmDyl2QcUG","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHUyRGtXQfcn"},"source":["lh_labels"],"id":"xHUyRGtXQfcn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NI5XQXIbQiOk"},"source":["rh_labels = np.zeros(32492, dtype='int')\n","# for (ii,myrow) in enumerate(lh_data):\n","for (ii,myrow) in enumerate(rh_data):\n","    mycutoff = rh_thresh_list[ii]\n","    mylabel = ii + 1 # we want labels to start at 1, not 0\n","    rh_labels[myrow > mycutoff] = mylabel"],"id":"NI5XQXIbQiOk","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWLjTXCyQlL5"},"source":["rh_labels"],"id":"sWLjTXCyQlL5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H899b0P6QnnC"},"source":["lh_hemi_LR32k = sub.hemis['lh_LR32k']\n","rh_hemi_LR32k = sub.hemis['rh_LR32k']\n","# We can make an ipyvolume figure to plot both hemispheres on:\n","fig = ipv.figure()\n","# Then plot each hemisphere using whichever ICA component we\n","# want to visualize:\n","ny.cortex_plot(lh_hemi_LR32k, surface='very_inflated',\n","               color=lh_labels, cmap='hot', figure=fig)\n","ny.cortex_plot(rh_hemi_LR32k, surface='very_inflated',\n","               color=rh_labels, cmap='hot', figure=fig)"],"id":"H899b0P6QnnC","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfn6gDd0cE4e"},"source":["<a name=\"optim\"></a>\n","# Optimization (Pradyumna)"],"id":"vfn6gDd0cE4e"},{"cell_type":"code","metadata":{"id":"a4-9h3joeNcs"},"source":["# Load the connecitvity data as a dataframe\n","DataDir= '/home/jovyan/data/connectivityml/HCP_PTN1200/netmats'\n","\n","# Load connecitivty data as dictionaries\n","Folders_list = os.listdir(DataDir)\n","Conn_data1 = dict();\n","Conn_data2 = dict();\n","j=0; Nodes = list();\n","for flds in Folders_list:\n","    Nodes.append(flds.split(\"_\")[3])\n","    net1 = np.loadtxt(os.path.join(DataDir,flds,'netmats1.txt'))\n","    net2 = np.loadtxt(os.path.join(DataDir,flds,'netmats2.txt'))\n","    N  = int(Nodes[j][1:])\n","    indx = np.triu_indices(N,1)\n","    mat1 = np.transpose(np.transpose(net1.reshape(1003,N,N),[1,2,0])[indx],[1,0])\n","    mat2 = np.transpose(np.transpose(net2.reshape(1003,N,N),[1,2,0])[indx],[1,0])\n","    Conn_data1[Nodes[j]] = mat1\n","    Conn_data2[Nodes[j]] = mat2\n","    j=j+1\n","\n","\n","\n","# Loading the behavioral data\n","Behv_data_df = pd.read_csv('/home/jovyan/ML-Proj/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n","\n","# Loading Subject IDs\n","SubjectID= np.loadtxt(\"/home/jovyan/ML-Proj/subjectIDs.txt\")"],"id":"a4-9h3joeNcs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O68ep4g9eQty"},"source":["#Loading and selecting subjects in behavioral data based on available RS data\n","Behv_data_df = pd.read_csv('/home/jovyan/ML-Proj/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n","Behv_data_df = Behv_data_df.set_index('Subject')\n","Behv_data_df = Behv_data_df.loc[SubjectID,:]"],"id":"O68ep4g9eQty","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClZEZOaOeTkx"},"source":["# Choosing a predictor (We chose gender)\n","y= Behv_data_df[\"Gender\"]\n","y= y.to_numpy()"],"id":"ClZEZOaOeTkx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ac7U5Vo6eVoc"},"source":["# Optimization using nested CV\n","\n","hyp1 = dict();\n","hyp2 = dict();\n","hyp1['C'] =[0.1,1,10] # Hyperparameters for SVM\n","hyp2['n_estimators'] = [1,10,100] # Hyperparameters for SVM\n","\n","SVM_results_IC1= dict();\n","RF_results_IC1= dict();\n","SVM_results_IC2= dict();\n","RF_results_IC2= dict();\n","\n","for j in np.arange(len(Folders_list)):\n","    X1= Conn_data1[Nodes[j]]\n","    X2= Conn_data2[Nodes[j]]\n","    clf1 = svm.SVC(kernel='linear')\n","    clf2 = RandomForestClassifier(max_depth =2)\n","    outer_cv = KFold(n_splits=3, shuffle=True)\n","    SVM_results1= list()\n","    RF_results1= list()\n","    SVM_results2= list()\n","    RF_results2= list()\n","    for train_ix, test_ix in outer_cv.split(X1):\n","        # split data into train, test for outer CV\n","        X1_train, X1_test = X1[train_ix], X1[test_ix]\n","        y_train, y_test = y[train_ix], y[test_ix]\n","        X2_train, X2_test = X2[train_ix], X2[test_ix]\n","        inner_cv=  KFold(n_splits=3, shuffle=True)       \n","        Search1 = GridSearchCV(estimator=clf1,scoring='accuracy', param_grid=hyp1, cv=inner_cv,refit=True)\n","        Search2 = GridSearchCV(estimator=clf2,scoring='accuracy', param_grid=hyp2, cv=inner_cv,refit=True)\n","        # execute search\n","        resultSVM1 =  Search1.fit(X1_train, y_train)\n","        resultRF1 =  Search2.fit(X1_train, y_train)\n","        resultSVM2 =  Search1.fit(X2_train, y_train)\n","        resultRF2 =  Search2.fit(X2_train, y_train)\n","        # get the best performing model fit on the whole training set\n","        best_SVMmodel1 = resultSVM1.best_estimator_\n","        best_SVMmodel2 = resultSVM2.best_estimator_\n","        best_RFmodel1 = resultRF1.best_estimator_\n","        best_RFmodel2 = resultRF2.best_estimator_\n","        # evaluate the best model on the hold out dataset\n","        yhatSVM1 = best_SVMmodel1.predict(X1_test)\n","        yhatRF1 = best_RFmodel1.predict(X1_test)\n","        yhatSVM2= best_SVMmodel2.predict(X2_test)\n","        yhatRF2= best_RFmodel2.predict(X2_test)\n","        # Calculate the accuracy\n","        SVMacc1= accuracy_score(y_test,yhatSVM1)\n","        RFacc1= accuracy_score(y_test, yhatRF1)\n","        SVMacc2= accuracy_score(y_test,yhatSVM2)\n","        RFacc2= accuracy_score(y_test, yhatRF2)\n","        # store the result for each method\n","        SVM_results1.append(SVMacc1)\n","        RF_results1.append(RFacc1)\n","        SVM_results2.append(SVMacc2)\n","        RF_results2.append(RFacc2)\n","        \n","    # store the results for all IC parcellations\n","    SVM_results_IC1[Nodes[j]]= SVM_results1\n","    RF_results_IC1[Nodes[j]]= RF_results1\n","    SVM_results_IC2[Nodes[j]]= SVM_results2\n","    RF_results_IC2[Nodes[j]]= RF_results2"],"id":"Ac7U5Vo6eVoc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IncfNjFAeYKd"},"source":["# Tabulating the results\n","SVMacc1 =  pd.DataFrame(SVM_results_IC1)\n","RFacc1 =  pd.DataFrame(RF_results_IC1)\n","SVMacc2 =  pd.DataFrame(SVM_results_IC2)\n","RFacc2=  pd.DataFrame(RF_results_IC2)\n","SVMacc1.index = hyp1['C']\n","SVMacc2.index = hyp1['C']\n","RFacc1.index = hyp2['n_estimators']\n","RFacc2.index = hyp2['n_estimators']\n","print(\"\\n Correlation\\n\")\n","print(\"SVM\\n\")\n","print(SVMacc1)\n","print(\"\\nRF\\n\")\n","print(RFacc1)\n","print(\"\\n Partial correlation\\n\")\n","print(\"SVM\\n\")\n","print(SVMacc2)\n","print(\"\\nRF\\n\")\n","print(RFacc2)"],"id":"IncfNjFAeYKd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtfDxbZreb93"},"source":["# Plotting the results using 3D barplots\n","fig = plt.figure(figsize=(10,10),dpi=80)\n","ax1= fig.add_subplot(2,2,1, projection='3d')\n","ax2= fig.add_subplot(2,2,2, projection='3d')\n","ax3= fig.add_subplot(2,2,3, projection='3d')\n","ax4= fig.add_subplot(2,2,4, projection='3d')\n","\n","lx1= len(hyp1['C'] )\n","ly = len(Nodes)\n","lx2= len(hyp2['n_estimators'])\n","\n","xpos1= np.arange(0, lx1,1)\n","xpos2= np.arange(0, lx2,1)\n","ypos= np.arange(0, ly, 1)\n","xpos1g,yposg=  np.meshgrid(xpos1,ypos)\n","xpos2g,yposg=  np.meshgrid(xpos2,ypos)\n","\n","\n","# Setting x, y and z positions\n","xpos1g = xpos1g.flatten()\n","yposg= yposg.flatten()\n","xpos2g = xpos2g.flatten()\n","zpos1g=np.zeros(lx1*ly)\n","zpos2g=np.zeros(lx2*ly)\n","\n","# Making the depths \n","dx1= 0.5*np.ones_like(zpos1g)\n","dx2= 0.5*np.ones_like(zpos2g)\n","dy1= 0.5*np.ones_like(zpos1g)\n","dy2= 0.5*np.ones_like(zpos2g)\n","\n","dz11=SVMacc1.values.flatten()\n","dz12=SVMacc2.values.flatten()\n","dz21=RFacc1.values.flatten()\n","dz22=RFacc2.values.flatten()\n","\n","\n","ax1.bar3d(xpos1g,yposg,zpos1g,dx1,dy1,dz11)\n","ax2.bar3d(xpos1g,yposg,zpos1g,dx1,dy1,dz12)\n","ax3.bar3d(xpos2g,yposg,zpos2g,dx2,dy2,dz21)\n","ax4.bar3d(xpos2g,yposg,zpos2g,dx2,dy2,dz22)\n","\n","ax1.w_xaxis.set_ticks(np.arange(lx1))\n","ax1.w_xaxis.set_ticklabels(hyp1['C'])\n","ax1.w_yaxis.set_ticks(np.arange(ly))\n","ax1.w_yaxis.set_ticklabels(Nodes)\n","\n","ax2.w_xaxis.set_ticks(np.arange(lx1))\n","ax2.w_xaxis.set_ticklabels(hyp1['C'])\n","ax2.w_yaxis.set_ticks(np.arange(ly))\n","ax2.w_yaxis.set_ticklabels(Nodes)\n","\n","ax3.w_xaxis.set_ticks(np.arange(lx2))\n","ax3.w_xaxis.set_ticklabels(hyp2['n_estimators'])\n","ax3.w_yaxis.set_ticks(np.arange(ly))\n","ax3.w_yaxis.set_ticklabels(Nodes)\n","\n","ax4.w_xaxis.set_ticks(np.arange(lx2))\n","ax4.w_xaxis.set_ticklabels(hyp2['n_estimators'])\n","ax4.w_yaxis.set_ticks(np.arange(ly))\n","ax4.w_yaxis.set_ticklabels(Nodes)\n","\n","\n","ax1.set_xlabel('C')\n","ax1.set_ylabel('IC type')\n","ax1.set_zlabel('Accuracy')\n","ax1.title.set_text('SVM-Corr')\n","\n","ax2.set_xlabel('C')\n","ax2.set_ylabel('IC type')\n","ax2.set_zlabel('Accuracy')\n","ax2.title.set_text('SVM-Pcorr')\n","\n","ax3.set_xlabel('No of trees')\n","ax3.set_ylabel('IC type')\n","ax3.set_zlabel('Accuracy')\n","ax3.title.set_text('RF-Corr')\n","\n","ax4.set_xlabel('No of trees')\n","ax4.set_ylabel('IC type')\n","ax4.set_zlabel('Accuracy')\n","ax4.title.set_text('RF-Pcorr')\n","\n","\n","plt.show()"],"id":"QtfDxbZreb93","execution_count":null,"outputs":[]}]}