{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"connectivity_ml-plainRFC-addPL-addYZ.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"tags":[],"id":"8d0bb83e-ce84-4d64-8eca-9c51a25ffec0"},"source":["# Using functional connectivity from the HCP to predict individual characteristics\n","\n","Connectivity ML Group @ [Neurohackademy 2021](https://neurohackademy.org/)\n","\n","**Table of Content**\n"," - Import\n"," - Load the data\n"," - Visualization\n"," - Optimization\n","\n"],"id":"8d0bb83e-ce84-4d64-8eca-9c51a25ffec0"},{"cell_type":"markdown","metadata":{"id":"ulV34LHNatFU"},"source":["# But first, import!"],"id":"ulV34LHNatFU"},{"cell_type":"code","metadata":{"id":"3b4fc317-59af-420f-95e3-d3b7aa4e48ec"},"source":["# our core libraries\n","import os # (from PL)\n","import math\n","import numpy as np\n","import pandas as pd\n","import neuropythy as ny\n","import nibabel as nib\n","import ipyvolume as ipv\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV # (from TPA and PL)\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix # (from TPA and PL)\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import LabelEncoder # (from PL)\n","from sklearn import svm # (from TPA and PL)\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.linear_model import SGDRegressor, LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier # (from TPA and PL)\n","\n","import shap  # package used to calculate Shap values\n","import itertools # (from PL)\n","import networkx as nx # (from YZ)\n","\n","# (from DB)\n","\n","# (from PK)\n"],"id":"3b4fc317-59af-420f-95e3-d3b7aa4e48ec","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"faf9b768-f82b-4531-a679-5e0b7e04e0e2"},"source":["# You need to configure neuropythy so that it knows what your\n","# HCP AWS S3 access key and secret are:\n","# key = 'YOURKEY'\n","# secret = 'YOURSECRET'\n","\n","ny.config['hcp_credentials'] = (key, secret)\n","\n","ny.config['hcp_auto_download'] = True\n","ny.config['hcp_auto_path'] = '~/hcp_data'"],"id":"faf9b768-f82b-4531-a679-5e0b7e04e0e2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"326428aa-16ff-4b62-a9f2-98f3ef6ea5a1"},"source":["# Next, load the data..\n","\n","To `netmaps_df` we load \"netmaps\" which are subject-specific “parcellated connectomes” – for each subject, a nodes x nodes network matrix. See more [here](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP1200-DenseConnectome+PTN+Appendix-July2017.pdf).\n","\n","To `behavioral_df` we load the data keys. See more [here](https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release). "],"id":"326428aa-16ff-4b62-a9f2-98f3ef6ea5a1"},{"cell_type":"code","metadata":{"id":"26bc00c4-a07a-4789-9c7f-b83c16b2ab1a"},"source":["N = 15 # number of ICAs - 15, 25, 50 ,100 ,200 , 300"],"id":"26bc00c4-a07a-4789-9c7f-b83c16b2ab1a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"074430fb-1f4e-4f53-a4c3-d6aedbab83a7"},"source":["netmaps_df = pd.read_csv('data/connectivityml/HCP_PTN1200/netmats/3T_HCP1200_MSMAll_d'+str(N)+'_ts2/netmats2.txt', delim_whitespace=True,header=None)\n","print(\"Network-matrices data shape:\", netmaps_df.shape)\n","netmaps_df.head()"],"id":"074430fb-1f4e-4f53-a4c3-d6aedbab83a7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fc5d3c6-7ae8-436e-bdbc-751ce857565e"},"source":["behavioral_df = pd.read_csv('data/connectivityml/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n","print(\"Behaviora data shape:\", behavioral_df.shape)\n","behavioral_df.head()"],"id":"8fc5d3c6-7ae8-436e-bdbc-751ce857565e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75939302-5820-423e-9861-1764c005a27c"},"source":["We have netmaps for 1003 subjects so we will need to filter `behavioral_df` a little.\n","\n","To `subjectsID_df` we load the ordered list of all subjects with complete rfMRI data (recon 1 + recon2) included in this PTN release"],"id":"75939302-5820-423e-9861-1764c005a27c"},{"cell_type":"code","metadata":{"id":"af7312c7-68a1-4307-a857-cc011cd8b962"},"source":["subjectsID_df = pd.read_csv('data/connectivityml/HCP_PTN1200/subjectIDs.txt',header=None,names=[\"Subject\"])\n","print(\"Subjects ID data shape:\", subjectsID_df.shape)\n","subjectsID_df.head()"],"id":"af7312c7-68a1-4307-a857-cc011cd8b962","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5b5c6a4-043c-4a1b-bcf1-b69317e468bc"},"source":["We can see that this corresponds to the number of netmaps we have."],"id":"b5b5c6a4-043c-4a1b-bcf1-b69317e468bc"},{"cell_type":"code","metadata":{"id":"80235878-c331-4f35-8d81-c1f3460abb9a"},"source":["filter_behavioral_df = subjectsID_df.merge(behavioral_df, on='Subject', how='inner')\n","\n","print(\"Filtered behaviora data shape:\", filter_behavioral_df.shape)\n","filter_behavioral_df.head()"],"id":"80235878-c331-4f35-8d81-c1f3460abb9a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"303e2bef-67b1-4ac2-b367-a1885c39165a"},"source":["## Pre-process features matrix"],"id":"303e2bef-67b1-4ac2-b367-a1885c39165a"},{"cell_type":"code","metadata":{"id":"339377cf-c709-465c-a10c-2c4c6e1ec327"},"source":["netmapsX_df = pd.DataFrame(data = netmaps_df, columns = range(N*N))\n","netmapsX_df = netmapsX_df.T.drop_duplicates(keep='first').T\n","netmapsX_df = netmapsX_df.T.drop_duplicates(keep='last').T\n","X = netmapsX_df\n","print(\"Features matrix shape:\", X.shape)\n","X.head()"],"id":"339377cf-c709-465c-a10c-2c4c6e1ec327","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0b9fc56c-a343-4b1f-a5c4-2de5997907c7"},"source":["## Pre-process predicted values\n","\n","Here we are going to foucs on the subject gender."],"id":"0b9fc56c-a343-4b1f-a5c4-2de5997907c7"},{"cell_type":"code","metadata":{"id":"aebe74da-cd7d-4bac-b929-8d546e82123f"},"source":["filter_behavioral_df['Gender_i']=np.zeros(shape=(subjectsID_df.shape))\n","filter_behavioral_df.Gender_i = pd.factorize(filter_behavioral_df.Gender)[0] # Encode the object as an enumerated type or categorical variable.\n","y_gender = filter_behavioral_df.Gender_i # Gender of Subject\n","print(\"y_gender shape:\", y_gender.shape)\n","\n","filter_behavioral_df['Gender_i'].groupby(filter_behavioral_df['Gender']).unique().apply(pd.Series).rename(columns={0:'Labels'}).sort_values(by='Labels')"],"id":"aebe74da-cd7d-4bac-b929-8d546e82123f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fc95d1f-d3cc-48ed-8f76-868b08ce27a7"},"source":["fig, ax = plt.subplots()\n","sns.histplot(y_gender, ax=ax)\n","ax.set_title(\"Gender of Subject\")\n","fig.tight_layout()"],"id":"6fc95d1f-d3cc-48ed-8f76-868b08ce27a7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"90a4cdb9-4090-40d3-af5d-28f3baf14b44"},"source":["## Time for Random forests!\n","\n","Data and estimators exploration was done in a separate notebook..\n","\n","### [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n","\n","A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n","\n","Here we will prdict the gender of subjects in our dataset from our network-matrices data has gave us the best results..."],"id":"90a4cdb9-4090-40d3-af5d-28f3baf14b44"},{"cell_type":"code","metadata":{"id":"e1bd76e4-94e6-4d87-9508-499c834ebda4"},"source":["xtrain, xtest, ytrain, ytest = train_test_split(X, y_gender, test_size=0.2,stratify=y_gender,random_state=1)"],"id":"e1bd76e4-94e6-4d87-9508-499c834ebda4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fa8858b-3164-4b22-bc03-568cdf1f5e6e"},"source":["rf = RandomForestClassifier(random_state=1)\n","\n","rf.fit(xtrain, ytrain) # Build a forest of trees from the training set (X, y).\n","\n","score = rf.score(xtest, ytest) # Return the mean accuracy on the given test data and labels.\n","print(\"Test set score: \", score) \n","\n","ypred = rf.predict(xtest) # Predict class for X."],"id":"4fa8858b-3164-4b22-bc03-568cdf1f5e6e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"523c7b72-0781-4ad5-aac4-553d70ccad58"},"source":["# View confusion matrix for test data and predictions\n","plot_confusion_matrix(rf, xtest, ytest) # Plot Confusion Matrix.\n","plt.savefig('output/confusion_matrix_'+str(N)+'.png')"],"id":"523c7b72-0781-4ad5-aac4-553d70ccad58","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3b139368-2e37-49f1-9b1d-dac6a36e2e42"},"source":["cr = classification_report(ytest, ypred,output_dict=True) # Build a text report showing the main classification metrics.\n","sns_plot = sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True) # plot scikit-learn classification report\n","sns_plot.figure.savefig(\"output/classification_report_\"+str(N)+\".png\")"],"id":"3b139368-2e37-49f1-9b1d-dac6a36e2e42","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ee545cbc-2457-49cd-96a4-8cfad5d22e37"},"source":["#### Visualizing trees\n","We can plot individual decision trees - "],"id":"ee545cbc-2457-49cd-96a4-8cfad5d22e37"},{"cell_type":"code","metadata":{"id":"a51dd795-c97d-4019-8c65-4392fb9ef964"},"source":["from sklearn.tree import export_graphviz\n","from subprocess import call\n","from IPython.display import Image, display\n","\n","def plot_graphviz_tree(tree):\n","    \"\"\"\n","    Helper function that takes a tree as input, calls sklearn's export_graphviz\n","    function to generate an image of the tree using graphviz, and then\n","    plots the result in-line.\n","    \"\"\"\n","    export_graphviz(tree, out_file='tree.dot', max_depth=3, filled=True,\n","                    feature_names=X.columns, impurity=False, rounded=True,\n","                    proportion=False, precision=2);\n","\n","    call(['dot', '-Tpng', 'tree.dot', '-o', 'output/tree_'+str(N)+'.png', '-Gdpi=600'])\n","    display(Image(filename = 'output/tree_'+str(N)+'.png'));"],"id":"a51dd795-c97d-4019-8c65-4392fb9ef964","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a063b39-ba0f-48be-b56d-a3af22f67501"},"source":["# First tree in the forest\n","plot_graphviz_tree(rf.estimators_[0]);"],"id":"8a063b39-ba0f-48be-b56d-a3af22f67501","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e93f20a0-5368-4bbb-89bb-0fdd857f62b4"},"source":["#### Interpreting random forests\n","\n","##### Feature importances\n","\n","Unlike regression-based methods, random forests don't have linear coefficientsso we look at the feature importances, to tell us how each feature contributes to the overall prediction."],"id":"e93f20a0-5368-4bbb-89bb-0fdd857f62b4"},{"cell_type":"code","metadata":{"id":"a9490afb-09cc-4bad-991b-fab07af60cb7"},"source":["f = open('output/feature_importances_'+'_'+str(N)+'.txt', \"w\")\n","f.write(str(pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)))\n","f.close()\n","\n","# plot feature importances\n","(pd.Series(rf.feature_importances_, index=X.columns)\n","   .nlargest(10)\n","   .plot(kind='bar',title=\"Feature importances for \"+str(N)+ \" ICA's\"))\n","plt.savefig('output/feature_importances_'+str(N)+'.png')"],"id":"a9490afb-09cc-4bad-991b-fab07af60cb7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3b931bfb-dcd4-40cf-b9e7-ba6567805726"},"source":["##### SHAP values\n","To interpret our results we will look at the SHAP values of our model.\n","See more [here](https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a).\n","\n","SHAP feature importance is an alternative to standard feature importance based on magnitude of feature attributions. The feature importance is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot.\n","\n","The SHAP summary plot is made of all the dots in the test data. Showing:\n","- **Feature importance:** Variables are ranked in descending order.\n","- **Impact:** The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n","- **Original value:** Color shows whether that variable is high (in red) or low (in blue) for that observation.\n"],"id":"3b931bfb-dcd4-40cf-b9e7-ba6567805726"},{"cell_type":"code","metadata":{"id":"ffc29fdb-57d3-4333-ad92-2e741f936898"},"source":["# Create object that can calculate shap values\n","explainer = shap.TreeExplainer(rf)\n","\n","# calculate shap values. This is what we will plot.\n","# Calculate shap_values for all of xtest rather than a single row, to have more data for plot.\n","shap_values = explainer.shap_values(xtest)\n","\n","# Make plot\n","fig = shap.summary_plot(shap_values[1], xtest, show=False)\n","plt.savefig('output/summary_plot_xtest_'+str(N)+'.png')"],"id":"ffc29fdb-57d3-4333-ad92-2e741f936898","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7ce6a5f-5963-4b41-9024-af6c7e832a8d"},"source":["fig = shap.summary_plot(shap_values, xtest, show=False,plot_type='bar')\n","plt.savefig('output/SHAP_feature_importances_xtest_'+str(N)+'.png')"],"id":"f7ce6a5f-5963-4b41-9024-af6c7e832a8d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c67658a9-a5df-4707-a2aa-c40db4ca6214"},"source":["#### Single subject interpretation\n","We will look at SHAP values for a random row of the dataset.\n","\n","##### SHAP values\n","For context, we'll look at the raw predictions before looking at the SHAP values."],"id":"c67658a9-a5df-4707-a2aa-c40db4ca6214"},{"cell_type":"code","metadata":{"id":"28411b66-fb7a-4200-8783-4abc9c5648e3"},"source":["row_to_show = 5\n","data_for_prediction = xtest.loc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n","data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n","\n","rf.predict_proba(data_for_prediction_array)"],"id":"28411b66-fb7a-4200-8783-4abc9c5648e3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dffefd2-c8dc-455a-a303-eacad55d9d79"},"source":["print(\"Subject evaluated:\",subjectsID_df.Subject[row_to_show])"],"id":"9dffefd2-c8dc-455a-a303-eacad55d9d79","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6447328a-4c58-4154-9aeb-27f5f5c31a9d"},"source":["# Create object that can calculate shap values\n","explainer = shap.TreeExplainer(rf)\n","\n","# Calculate Shap values\n","shap_values = explainer.shap_values(data_for_prediction)\n","shap.initjs()\n","\n","shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=False)\n","# plt.savefig('force_plot_hcp_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.png')"],"id":"6447328a-4c58-4154-9aeb-27f5f5c31a9d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"11c07ff3-e721-43b9-b6f9-e1419f780a47"},"source":["The SHAP values of all features sum up to explain why the prediction was different from the baseline. This allows us to decompose a prediction in a graph like this where:\n","\n","- The **output value** is the prediction for that observation (the prediction of the subject evluated).\n"," - The **base value** is the value that would be predicted if we did not know any features for the current output (the mean prediction, or mean(yhat)).\n"," - **Red/blue:** Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.\n"," \n","##### TOP 5 Most important ICA's"],"id":"11c07ff3-e721-43b9-b6f9-e1419f780a47"},{"cell_type":"code","metadata":{"id":"1d1588e0-56c4-4129-9daf-590afb7ea7ba"},"source":["vals= np.abs(shap_values).mean(0)\n","feature_importance = pd.DataFrame(list(zip(xtest.columns,vals)),columns=['netmat_col','feature_importance_vals'])\n","feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n","feature_importance.head()"],"id":"1d1588e0-56c4-4129-9daf-590afb7ea7ba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cdd31dd-50e0-48d2-8543-6fca0eaa24b4"},"source":["edge_df = pd.DataFrame(columns = ['netmat_col','Node1', 'Node2', 'Weight'])\n","countlist = list() # if node1,node2 weight is saved no need to save node2,node1\n","\n","for index, row in tqdm(netmaps_df.iterrows(), total=netmaps_df.shape[0]):\n","    netmat_col = 0\n","    if index == row_to_show:\n","        row2mat = row.values.reshape(N,N)\n","        for node1 in range(N):\n","            for node2 in range(N):\n","                if node1!=node2:\n","                    if (node2, node1) not in countlist:\n","                        countlist.append((node1, node2))\n","                        curr_edge = {'netmat_col': netmat_col, 'Node1': node1, 'Node2': node2, 'Weight':row2mat[node1][node2]}\n","                        edge_df = edge_df.append(curr_edge, ignore_index = True)\n","                netmat_col = netmat_col + 1"],"id":"2cdd31dd-50e0-48d2-8543-6fca0eaa24b4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2571ad6-12ed-4e5b-a6d0-02e721e13122"},"source":["f = open('output/top5_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.txt', \"w\")\n","\n","for i in range(5):\n","    feature = feature_importance.netmat_col.values[i]\n","    print(\"Important feature #: \",feature)\n","    f.write(\"Important feature #: \" + str(feature)+\"\\n\")\n","    ICA_Node1 = edge_df.loc[edge_df['netmat_col'] == feature, 'Node1']\n","    ICA_Node2 = edge_df.loc[edge_df['netmat_col'] == feature, 'Node2']\n","\n","    print(\"Node 1:\",ICA_Node1.iloc[0])\n","    f.write(\"Node 1: \" + str(ICA_Node1.iloc[0])+\"\\n\")\n","    print(\"Node 2:\",ICA_Node2.iloc[0])\n","    f.write(\"Node 2: \" + str(ICA_Node2.iloc[0])+\"\\n\")\n","f.close()"],"id":"a2571ad6-12ed-4e5b-a6d0-02e721e13122","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9990cff1-c03e-417a-99c8-ba6db5dfe9c1"},"source":["Knowing the ICA's that are part of the most important feature we can plot them on our single subject brain"],"id":"9990cff1-c03e-417a-99c8-ba6db5dfe9c1"},{"cell_type":"code","metadata":{"id":"aad6774d-f6d6-40e4-891f-cee62e01c4c1"},"source":["# Get a sample HCP subject:\n","sub = ny.hcp_subject(subjectsID_df.Subject[row_to_show])\n","sub"],"id":"aad6774d-f6d6-40e4-891f-cee62e01c4c1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1585ddd9-9f6b-42e4-9f1c-291798da2673"},"source":["# Load the CIFTI file:\n","cii_filename = '~/data/connectivityml/HCP_PTN1200/groupICA/groupICA_3T_HCP1200_MSMAll_d'+str(N)+'.ica/melodic_IC.dscalar.nii'\n","cii_obj = ny.load(cii_filename)\n","\n","# Split the CIFTI object into hemisphere/subvoxel data:\n","(lh_data, rh_data, subvox_data) = ny.hcp.cifti_split(cii_obj)\n","\n","# These data should be (N(data-points) x vertices)\n","lh_data.shape"],"id":"1585ddd9-9f6b-42e4-9f1c-291798da2673","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6570e87-ee28-4383-93de-b3eab9caf9c7"},"source":["# sub.lh and sub.rh are the \"native\" (FreeSurfer) hemispheres;\n","# sub.hemis['lh_LR32k'] and sub.hemis['rh_LR32k'] are the\n","# HCP subject-aligned fs_LR hemispheres (with 32k resolution).\n","lh_hemi_native = sub.lh\n","rh_hemi_native = sub.rh\n","\n","# The 32492 size indicates this is a 32k LR hemisphere:\n","lh_hemi = sub.hemis['lh_LR32k']\n","rh_hemi = sub.hemis['rh_LR32k']"],"id":"a6570e87-ee28-4383-93de-b3eab9caf9c7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"ea1aa2b6-bf67-4851-8dfb-ad78c567717b"},"source":["**ICA 1**"],"id":"ea1aa2b6-bf67-4851-8dfb-ad78c567717b"},{"cell_type":"code","metadata":{"id":"358687d7-4423-47d0-9782-a6d9a5af23ee"},"source":["# We can make an ipyvolume figure to plot both hemispheres on:\n","fig = ipv.figure()\n","# Then plot each hemisphere using whichever ICA component we\n","# want to visualize:\n","\n","ICA_Node1 = int(edge_df.loc[edge_df['netmat_col'] == feature_importance.netmat_col.values[0], 'Node1'])\n","ny.cortex_plot(lh_hemi, surface='inflated', color=lh_data[ICA_Node1], cmap='hot', figure=fig)\n","ny.cortex_plot(rh_hemi, surface='inflated', color=rh_data[ICA_Node1], cmap='hot', figure=fig)"],"id":"358687d7-4423-47d0-9782-a6d9a5af23ee","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7c08b73-3852-456d-bdbe-269ce071c14e"},"source":["ipv.pylab.save('output/most_important_ICA1_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.html',title='Most important feature ICA 1')"],"id":"f7c08b73-3852-456d-bdbe-269ce071c14e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"bef74681-caca-4c3b-9bba-48225091d407"},"source":["**ICA 2**"],"id":"bef74681-caca-4c3b-9bba-48225091d407"},{"cell_type":"code","metadata":{"id":"23b91b7a-95d0-4437-87e2-25141ee7b8ec"},"source":["# We can make an ipyvolume figure to plot both hemispheres on:\n","fig = ipv.figure()\n","# Then plot each hemisphere using whichever ICA component we\n","# want to visualize:\n","\n","ICA_Node2 = int(edge_df.loc[edge_df['netmat_col'] == feature_importance.netmat_col.values[0], 'Node2'])\n","ny.cortex_plot(lh_hemi, surface='inflated', color=lh_data[ICA_Node2], cmap='hot', figure=fig)\n","ny.cortex_plot(rh_hemi, surface='inflated', color=rh_data[ICA_Node2], cmap='hot', figure=fig)"],"id":"23b91b7a-95d0-4437-87e2-25141ee7b8ec","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adf994a1-a6d5-4161-8d08-7d20fcd92951"},"source":["ipv.pylab.save('output/most_important_ICA2_'+str(subjectsID_df.Subject[row_to_show])+'_'+str(N)+'.html',title='Most important feature ICA 2')"],"id":"adf994a1-a6d5-4161-8d08-7d20fcd92951","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttEkq-kJbfIk"},"source":["# Visualization"],"id":"ttEkq-kJbfIk"},{"cell_type":"markdown","metadata":{"id":"NjDqYhELbjAH"},"source":["## Brain-free network model using Gephi\n","\n","See more about Gephi [here](https://gephi.org/). The following primarily follows [this instruction](https://towardsdatascience.com/from-csv-to-github-pages-in-5-steps-publishing-an-interactive-social-network-of-the-marvel-7b8374bf44fb). There is a tutorial about using the `GephiStreamer` python package [here](https://pypi.org/project/GephiStreamer/), but that has to be run with Graph streaming plugin enabled, although there is an [instruction](https://tbgraph.wordpress.com/2017/04/01/neo4j-to-gephi/) it's not straightforward to set this up in NeuroHackademy Jupyterhub."],"id":"NjDqYhELbjAH"},{"cell_type":"code","metadata":{"id":"3346821a-069b-4400-9d61-b4b05e8f538e"},"source":["edge_df['weight'] = 1\n","\n","for ind in (feature_importance.netmat_col[:5].index):\n","    print(ind)\n","    edge_df.loc[ind-1,'weight'] = 5\n","edge_df"],"id":"3346821a-069b-4400-9d61-b4b05e8f538e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0JZ9sk4rhrgm"},"source":["Make sure that the weights from the top features are changed correctly for visualization:"],"id":"0JZ9sk4rhrgm"},{"cell_type":"code","metadata":{"id":"mA1WEE1Nhk3-"},"source":["edge_df.loc[edge_df['netmat_col'].isin(feature_importance.netmat_col[:5])]"],"id":"mA1WEE1Nhk3-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Av_u6KBviD1A"},"source":["G=nx.from_pandas_edgelist(edge_df, 'Node1', 'Node2', edge_attr='weight')"],"id":"Av_u6KBviD1A","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4Z9rgnTiXUq"},"source":["# number of ICAs - 15, 25, 50 ,100 ,200 , 300\n","\n","nx.write_gexf(G, \"/home/jovyan/Hackathon/code/tpatpa/connectivityml/gephi-15-top5-weighted.gexf\")"],"id":"j4Z9rgnTiXUq","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mwPoY1ljLDZ"},"source":["![Top 5 features from 15 ICAs, labelled](visualization-gephi/gephi-15-top5-weighted-labelled.png)"],"id":"2mwPoY1ljLDZ"},{"cell_type":"markdown","metadata":{"id":"KHLFeMISbr8y"},"source":["## Brain Mapping using Neuropythy"],"id":"KHLFeMISbr8y"},{"cell_type":"code","metadata":{"id":"UiamSex0jpLj"},"source":["# To be added"],"id":"UiamSex0jpLj","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfn6gDd0cE4e"},"source":["# Optimization"],"id":"vfn6gDd0cE4e"},{"cell_type":"code","metadata":{"id":"a4-9h3joeNcs"},"source":["# Load the connecitvity data as a dataframe\n","DataDir= '/home/jovyan/data/connectivityml/HCP_PTN1200/netmats'\n","\n","# Load connecitivty data as dictionaries\n","Folders_list = os.listdir(DataDir)\n","Conn_data1 = dict();\n","Conn_data2 = dict();\n","j=0; Nodes = list();\n","for flds in Folders_list:\n","    Nodes.append(flds.split(\"_\")[3])\n","    net1 = np.loadtxt(os.path.join(DataDir,flds,'netmats1.txt'))\n","    net2 = np.loadtxt(os.path.join(DataDir,flds,'netmats2.txt'))\n","    N  = int(Nodes[j][1:])\n","    indx = np.triu_indices(N,1)\n","    mat1 = np.transpose(np.transpose(net1.reshape(1003,N,N),[1,2,0])[indx],[1,0])\n","    mat2 = np.transpose(np.transpose(net2.reshape(1003,N,N),[1,2,0])[indx],[1,0])\n","    Conn_data1[Nodes[j]] = mat1\n","    Conn_data2[Nodes[j]] = mat2\n","    j=j+1\n","\n","\n","\n","# Loading the behavioral data\n","Behv_data_df = pd.read_csv('/home/jovyan/ML-Proj/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n","\n","# Loading Subject IDs\n","SubjectID= np.loadtxt(\"/home/jovyan/ML-Proj/subjectIDs.txt\")"],"id":"a4-9h3joeNcs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O68ep4g9eQty"},"source":["#Loading and selecting subjects in behavioral data based on available RS data\n","Behv_data_df = pd.read_csv('/home/jovyan/ML-Proj/unrestricted_pkalra_7_26_2021_17_39_25.csv')\n","Behv_data_df = Behv_data_df.set_index('Subject')\n","Behv_data_df = Behv_data_df.loc[SubjectID,:]"],"id":"O68ep4g9eQty","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClZEZOaOeTkx"},"source":["# Choosing a predictor (We chose gender)\n","y= Behv_data_df[\"Gender\"]\n","y= y.to_numpy()"],"id":"ClZEZOaOeTkx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ac7U5Vo6eVoc"},"source":["# Optimization using nested CV\n","\n","hyp1 = dict();\n","hyp2 = dict();\n","hyp1['C'] =[0.1,1,10] # Hyperparameters for SVM\n","hyp2['n_estimators'] = [1,10,100] # Hyperparameters for SVM\n","\n","SVM_results_IC1= dict();\n","RF_results_IC1= dict();\n","SVM_results_IC2= dict();\n","RF_results_IC2= dict();\n","\n","for j in np.arange(len(Folders_list)):\n","    X1= Conn_data1[Nodes[j]]\n","    X2= Conn_data2[Nodes[j]]\n","    clf1 = svm.SVC(kernel='linear')\n","    clf2 = RandomForestClassifier(max_depth =2)\n","    outer_cv = KFold(n_splits=3, shuffle=True)\n","    SVM_results1= list()\n","    RF_results1= list()\n","    SVM_results2= list()\n","    RF_results2= list()\n","    for train_ix, test_ix in outer_cv.split(X1):\n","        # split data into train, test for outer CV\n","        X1_train, X1_test = X1[train_ix], X1[test_ix]\n","        y_train, y_test = y[train_ix], y[test_ix]\n","        X2_train, X2_test = X2[train_ix], X2[test_ix]\n","        inner_cv=  KFold(n_splits=3, shuffle=True)       \n","        Search1 = GridSearchCV(estimator=clf1,scoring='accuracy', param_grid=hyp1, cv=inner_cv,refit=True)\n","        Search2 = GridSearchCV(estimator=clf2,scoring='accuracy', param_grid=hyp2, cv=inner_cv,refit=True)\n","        # execute search\n","        resultSVM1 =  Search1.fit(X1_train, y_train)\n","        resultRF1 =  Search2.fit(X1_train, y_train)\n","        resultSVM2 =  Search1.fit(X2_train, y_train)\n","        resultRF2 =  Search2.fit(X2_train, y_train)\n","        # get the best performing model fit on the whole training set\n","        best_SVMmodel1 = resultSVM1.best_estimator_\n","        best_SVMmodel2 = resultSVM2.best_estimator_\n","        best_RFmodel1 = resultRF1.best_estimator_\n","        best_RFmodel2 = resultRF2.best_estimator_\n","        # evaluate the best model on the hold out dataset\n","        yhatSVM1 = best_SVMmodel1.predict(X1_test)\n","        yhatRF1 = best_RFmodel1.predict(X1_test)\n","        yhatSVM2= best_SVMmodel2.predict(X2_test)\n","        yhatRF2= best_RFmodel2.predict(X2_test)\n","        # Calculate the accuracy\n","        SVMacc1= accuracy_score(y_test,yhatSVM1)\n","        RFacc1= accuracy_score(y_test, yhatRF1)\n","        SVMacc2= accuracy_score(y_test,yhatSVM2)\n","        RFacc2= accuracy_score(y_test, yhatRF2)\n","        # store the result for each method\n","        SVM_results1.append(SVMacc1)\n","        RF_results1.append(RFacc1)\n","        SVM_results2.append(SVMacc2)\n","        RF_results2.append(RFacc2)\n","        \n","    # store the results for all IC parcellations\n","    SVM_results_IC1[Nodes[j]]= SVM_results1\n","    RF_results_IC1[Nodes[j]]= RF_results1\n","    SVM_results_IC2[Nodes[j]]= SVM_results2\n","    RF_results_IC2[Nodes[j]]= RF_results2"],"id":"Ac7U5Vo6eVoc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IncfNjFAeYKd"},"source":["# Tabulating the results\n","SVMacc1 =  pd.DataFrame(SVM_results_IC1)\n","RFacc1 =  pd.DataFrame(RF_results_IC1)\n","SVMacc2 =  pd.DataFrame(SVM_results_IC2)\n","RFacc2=  pd.DataFrame(RF_results_IC2)\n","SVMacc1.index = hyp1['C']\n","SVMacc2.index = hyp1['C']\n","RFacc1.index = hyp2['n_estimators']\n","RFacc2.index = hyp2['n_estimators']\n","print(\"\\n Correlation\\n\")\n","print(\"SVM\\n\")\n","print(SVMacc1)\n","print(\"\\nRF\\n\")\n","print(RFacc1)\n","print(\"\\n Partial correlation\\n\")\n","print(\"SVM\\n\")\n","print(SVMacc2)\n","print(\"\\nRF\\n\")\n","print(RFacc2)"],"id":"IncfNjFAeYKd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtfDxbZreb93"},"source":["# Plotting the results using 3D barplots\n","fig = plt.figure(figsize=(10,10),dpi=80)\n","ax1= fig.add_subplot(2,2,1, projection='3d')\n","ax2= fig.add_subplot(2,2,2, projection='3d')\n","ax3= fig.add_subplot(2,2,3, projection='3d')\n","ax4= fig.add_subplot(2,2,4, projection='3d')\n","\n","lx1= len(hyp1['C'] )\n","ly = len(Nodes)\n","lx2= len(hyp2['n_estimators'])\n","\n","xpos1= np.arange(0, lx1,1)\n","xpos2= np.arange(0, lx2,1)\n","ypos= np.arange(0, ly, 1)\n","xpos1g,yposg=  np.meshgrid(xpos1,ypos)\n","xpos2g,yposg=  np.meshgrid(xpos2,ypos)\n","\n","\n","# Setting x, y and z positions\n","xpos1g = xpos1g.flatten()\n","yposg= yposg.flatten()\n","xpos2g = xpos2g.flatten()\n","zpos1g=np.zeros(lx1*ly)\n","zpos2g=np.zeros(lx2*ly)\n","\n","# Making the depths \n","dx1= 0.5*np.ones_like(zpos1g)\n","dx2= 0.5*np.ones_like(zpos2g)\n","dy1= 0.5*np.ones_like(zpos1g)\n","dy2= 0.5*np.ones_like(zpos2g)\n","\n","dz11=SVMacc1.values.flatten()\n","dz12=SVMacc2.values.flatten()\n","dz21=RFacc1.values.flatten()\n","dz22=RFacc2.values.flatten()\n","\n","\n","ax1.bar3d(xpos1g,yposg,zpos1g,dx1,dy1,dz11)\n","ax2.bar3d(xpos1g,yposg,zpos1g,dx1,dy1,dz12)\n","ax3.bar3d(xpos2g,yposg,zpos2g,dx2,dy2,dz21)\n","ax4.bar3d(xpos2g,yposg,zpos2g,dx2,dy2,dz22)\n","\n","ax1.w_xaxis.set_ticks(np.arange(lx1))\n","ax1.w_xaxis.set_ticklabels(hyp1['C'])\n","ax1.w_yaxis.set_ticks(np.arange(ly))\n","ax1.w_yaxis.set_ticklabels(Nodes)\n","\n","ax2.w_xaxis.set_ticks(np.arange(lx1))\n","ax2.w_xaxis.set_ticklabels(hyp1['C'])\n","ax2.w_yaxis.set_ticks(np.arange(ly))\n","ax2.w_yaxis.set_ticklabels(Nodes)\n","\n","ax3.w_xaxis.set_ticks(np.arange(lx2))\n","ax3.w_xaxis.set_ticklabels(hyp2['n_estimators'])\n","ax3.w_yaxis.set_ticks(np.arange(ly))\n","ax3.w_yaxis.set_ticklabels(Nodes)\n","\n","ax4.w_xaxis.set_ticks(np.arange(lx2))\n","ax4.w_xaxis.set_ticklabels(hyp2['n_estimators'])\n","ax4.w_yaxis.set_ticks(np.arange(ly))\n","ax4.w_yaxis.set_ticklabels(Nodes)\n","\n","\n","ax1.set_xlabel('C')\n","ax1.set_ylabel('IC type')\n","ax1.set_zlabel('Accuracy')\n","ax1.title.set_text('SVM-Corr')\n","\n","ax2.set_xlabel('C')\n","ax2.set_ylabel('IC type')\n","ax2.set_zlabel('Accuracy')\n","ax2.title.set_text('SVM-Pcorr')\n","\n","ax3.set_xlabel('No of trees')\n","ax3.set_ylabel('IC type')\n","ax3.set_zlabel('Accuracy')\n","ax3.title.set_text('RF-Corr')\n","\n","ax4.set_xlabel('No of trees')\n","ax4.set_ylabel('IC type')\n","ax4.set_zlabel('Accuracy')\n","ax4.title.set_text('RF-Pcorr')\n","\n","\n","plt.show()"],"id":"QtfDxbZreb93","execution_count":null,"outputs":[]}]}